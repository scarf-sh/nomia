#+OPTIONS: H:5
#+OPTIONS: toc:nil

# TODO title
* S High level executive summary
* Deep dive into Elvysh
#+BEGIN_QUOTE
There are only two hard things in Computer Science: cache invalidation and naming things.

  ---Phil Karlton
#+END_QUOTE
Naming and substitution are ubiquitous[fn:church] in computation, and many systems end up dealing with them explicitly. Compilers take module names and substitute in appropriate symbol tables. Browsers take URLs and substitute in appropriate web sites. Package managers take package names and substitute in appropriate changes to your environment. In other cases, the system has no explicit handle on names but the user or programmer fills in: We refer to other pieces of code, or techniques, or other computations, or data sources, or a million other things by name, and in implementation fill in special-case substitution of that name that preserves the intended meaning[fn:hope].

These domans are, of course, very different. Browsers don't know what a symbol table is, and installing a package is distinct from translating source code to object code. But there are many conceptual commonalities between them, commonalities which in principle could allow for shared implementation and semantics. Unfortunately, most of the time the required functionality is reimplemented from scratch. Like any missed opportunity for reuse, this duplicates work and bugs, leaves many implementations incomplete with respect to functionality or performance, and increases cognitive overhead for users and developers. In this case we also miss opportunities for *cross*-system composition: My package manager may know how to install libpq, and my compiler may know how to resolve libpq-fe.h to a library once it's installed, but there's no general-purpose way to note that the one name links to the other. With the commonalities abstracted into a shared component, system implementers can focus on their domain expertise, and users can benefit from correctness, efficiency, and a coherent, easy to use experience across all of their systems.

You're about to start working on a service that you haven't worked on before. You point your editor to the source file you need to work on, and after a slight delay for it to be downloaded for the first time you have it open in front of you. You start modifying the code, and after a minute or two for the library dependencies to be downloaded syntax checking kicks in and points out a typo a few lines above. You save your change and jump over to another file that depends on it (which opens immediately) and start making changes there. At first your editor warns you it's not up-to-date but there's an undefined references when you mentionend the new code you added, but after a few seconds for the first module to compile those errors go away. You finish the work and point your browser to the local service URL and get a page saying that the service is being spun up and showing the progress on that. You notice a DB dump is being loaded and you know that will take a while, so you switch to another project, a compute pipeline. When you enter its environment you notice a huge merge happened overnight, you were up to date with master so you don't have any conflicts but you're not sure if the merge impacted the parts of the codebase you care about. A colleague recommends you try out Meld to review the diff, you run a command to launch it and (after a minute of downloading, since you've never used it before) the window pops open and you confirm the merge was fine. You're testing out an algorithm tweak that should only impact a small portion of the parallel work units of the pipeline, after you finish implementation you kick off a run of the pipeline expected to be the same as that day's daily prod run except with just your changes in place, dry-running first to validate that only a small subset of the job is impacted. While that's running, you switch back to your browser and see the service is loaded, so you check your work and, satisfied, open a PR, which triggers an automention of the QA team with a URL to test out. You notice the pipeline run isn't quite done, so you turn off your computer and head out to lunch. When you get back, you see that the QA team reviewed your fix and it looks good and that the pipeline run is done, so you merge your changes to the first project and open up a comparison of the real prod run's results with your test run. After confirming the new results seem better, you open a PR with your proposed changes and head home.

To some of you, this may sound like a utopian dream. To others, a secret nightmare where all of the magic and implicit assumptions will inevitably cause a catastrophic break or, worse, subtle bugs missed until it's too late. A lucky few might have some subset of this available in some form in their domain. No one has it all... yet.

The vision outlined here is not impossible. It's not inherently unreliable, or brittle, or limited to a few special use cases. With appropriate use of names and a common system for substitution, we can dramatically reduce manual work, increase efficiency, and ensure correctness in almost any domain where computers are used. Elvysh provides the shared conceptual model and common implementation to make it real.

[fn:church] If you take the [[https://en.wikipedia.org/wiki/Lambda_calculus][Church]] side of the [[https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis][Church-Turing thesis]], name substitution is what computation *is*.
[fn:hope] We hope!
** TODO Link to model/impl sections?
+ Subtypes for structured editing?
** The conceptual model
Elvysh models[fn:cat] several core concepts and their interrelations[fn:mon]. In this section, we'll revisit the developer's dream from the introduction and progressively build up the model as a lens to understand one way it might have happened.

[fn:cat] Elvysh's model is based off of structures borrowed from category theory. No category theory is needed to understand this section, but footnotes will be included for those with the background.
[fn:mon] Many of the concepts come together to form a particular kind of monoidal 2-category
*** Resources
In your hypothetical work day, you exploited a number of *resources*: The source files you edited, the modules you imported, the web service you tested. Your usage took advantage of various *affordances* exposed via *capabilities* (or *handles*): Your shell /executed/ meld via its /path in a directory/, the pipeline run comparison tool used /connections to the respective results tables/ to /read/ in the results (or maybe the results directly afford higher level comparison primitives).

The individual resources used can be seen as instances of *resource types*[fn:0-cell]: the piece of the algorithm you modified can be seen as a /pipeline component/, the site the QA team evaluated was a /test web service/. Resource types associate *semantics* to the affordances of their inhabitants, often in terms of abstract *resource state*: an immutable readable file has its /contents/, a (finite) sequence of bytes, and sequential reads of the file return successive portions of those contents. Resource types also induce *equivalence classes* on their resources: Your dry-run of the pipeline confirmed that most of the results of your test run were equivalent (as compute results!) to the corresponding results of the prod run that day. The same resource can be seen as a member of multiple types: Your editor treated the first file you edited as a mutable single-writer regular file, while your compiler may have viewed it as a sequentially readable file. Some types are *subtypes* of others in that a resource of the subtype can also be seen as a resource of the *supertype*, in a way that preserves affordances but not necessarily semantics or equivalence: You can call "stat" on any Unix file and any immutable readable Unix file, but Unix files in general have no notion of contents (think of a socket) and two equivalent immutable readable files may have e.g. different inode numbers and thus be seen as different as Unix files.

Note that resource types and their associated semantics can be very domain-specific. Suppose the compute pipeline is written in C++ and your CI system uses gcc for performance but you prefer clang locally for the better error messages. The object files produced by the two compilers can be quite different, even viewed as object files, and so naïvely the object files compiled by CI after the big merge wouldn't be equivalent to the object files you'd compile locally. But viewed as "object files exporting the right symbols following the right platform ABI based on the relevant headers", they can be considered the same.

[fn:0-cell] The (generators of the) 0-cells of the category. Note that we do not in general identify a specific resource with some point of the relevant 0-cell, in part because there is no 1:1 mapping between a resource and its type and in part for reasons detailed in the next section.
*** Names
Each of the resources you utilized were referenced by a *name*[fn:1-cell]: "meld" names a particular program, "the test site for the PR you opened" names a particular web service. More generally, a name can be a procedure relating a finite sequence of resource types (its *inputs*) to a resulting sequence of resource types (its *outputs*)[fn:domcod]: "the Acme webservice" might be a name that relates inputs like an executable for the service, a database, and a service config file to an output web service. We can visualize that as:

[FIG]

Names must be *deterministic*, in the sense that the process must produce equivalent outputs when provided equivalent inputs. This may seem to make them too strict to be useful, but there are two mitigating factors. First, recall that equivalence is a domain-specific notion; depending on how high level the output type equivalences are the name may have quite a bit of leeway in exactly how it instantiates the desired resources. Moreover, names can be *contextual* (or *indexical*), meaning that their output can depend on (some aspect of) the caller's context; "the results of today's prod pipeline run" depends on what "today" means. This is modelled by a *context* resource types at the input, which can be thought of as specific subsets of "the state of the world from some particular perspective"; they can conceptually be instantiated with a (unique) instance from a caller outside the system or forwarded on (possibly after transformations) within it. Because each top-level instantiation is unique, contextual names are essentially unrestricted with respect to determinism, so long as the lack of determinism can be captured in the context.

Names with an empty list of inputs are therefore called *named resources*[fn:points], since they correspond directly to the (unique up to equivalence) resources produced when the name is run.

We can combine names via *substitution*[fn:1-comp], instantiating some input resources of one name with (subset *projections* of) the outputs of some other names (and so on recursively), resulting in a new name. We might visualize "the Acme webservice using the executable compiled from the latest code, the pristine test db, and some provided config file" as:

[FIG]

Which as a whole can be seen as new contextual name taking a config file as an input:

[FIG]

Names are *referentially transparent*[fn:cut-elim], in that we can replace a substitution by "inlining" the result resource rather than referencing it and get the same output (this follows from determinism).

Resource subtyping can be captured in *coercions* (or *upcasts*), names that map a single input to a single output and are operationally noops. The server compliation process coerced the writeable file your editor was using to a readable stream to generate an updated server executable.

Because of determinism, using names forces us to say exactly what we mean. Domain-specificity and contextuality /allow/ us to say exactly what we mean, and no stricter, especially if the contextual inputs are fine-grained. Together, this gives us an expressive specification that lets us rely on names and know what to expect with the resulting resources, across domains, modulo implementation bugs. Determinism also allows for efficient resource instantiation: If we can cheaply determine that the inputs are all equivalent to some previous instantiation (here or elsewhere), we can safely reuse the previous result, and to the extent that contextuality doesn't tie us to a specific machine we can safely distribute the work to other systems and take the result back when done. For named resources in particular, since the inputs are always vacuously equivalent we can aggressively cache and distribute them.

Many names can themselves be cheaply compared for equality. This allows for composed names to be subject to caching without necessarily running intermediate names or even instantiating their results from a cache, since if we know the top-level inputs are equivalent and each name in the chain is equivalent we know the outputs will be equivalent.

[fn:1-cell] The 1-cells of the category.
[fn:domcod] The domain and codomain of the 1-cells. Note that this could in principle be independently extended to a dependent multicategory by allowing the output types to depend on the specific input resources provided or to a codependent multicategory by allowing the inputs to vary depending on how the outputs are used, but there is currently no known practical use case for those.
[fn:points] /These/ are the points of the relevant 0-cell. Not every resource has a name that fits the requirements of names generally, at least not obviously so, so while every named resource corresponds to some resource the converse isn't true.
[fn:1-comp] This is (unbiased) composition of the 1-cells, including tensoring (i.e. projections).
[fn:cut-elim] This is "cut elimination" of the underlying multicategory

**** Technical note: Structural rules

The rules for names given so far imply very strict resource management: Every resource must be used, exactly once, in order. There are some cases where this is necessary for correctness. Consider the case where a name depends on three input streams that gets instantiated with three pipes each filled sequentially by the same process; the first pipe must be completely read from before the process will start filling the second one, so the name must consume it first, and the data streams can be arbitrarily long so they cannot in general be duplicated. In most cases, however, we can relax this through any combination of the following three schemes for *structural names*:

[FIG]

*Weakening* lets you ignore some resource: the name doesn't do anything with its input. *Contracting* lets you duplicate some input: the name copies[fn:ref] the resource it's instantiated with and sends one copy over each output. *Exchanging*, which can also be visualized by simply crossing wires, lets you reorder inputs: the input on the new left wire is forwarded on to the right output wire etc.

By default, all inputs and outputs are eligible for all three schemes. On a case by case basis we can conceptually annotate given inputs or outputs with *substructural restrictions*. Marking an output as *relevant* indicates that the result must be used and thus can't be weakened; marking an input as relevant indicates that the name does in fact use that input (e.g. internally it doesn't weaken it anywhere). Marking an output as *affine* indicates that the result can't be copied and thus can't be contracted; marking an input as affine indicates that the name does not duplicate that input. Marking an output as *ordered* indicates that nothing before it can be used once it's used (if ever) and it can't be used once something after has been used and thus can't be exchanged; marking an input as ordered indicates that the name does not reorder resources around that input[fn:one-sided].

In addition to ensuring correctness in rare cases, these annotations can also be used for optimization. If an input is marked relevant, the caller (or general substitution mechanism) might eagerly prepare the resource for consumption (e.g. starting a socket-activated service) rather than waiting for it to be used, since it will be eventually. If an input is marked affine, the caller might garbage collect the resource once it's used. If it's marked ordered, all resources before the input in question can be discarded/preparations stopped once the input is used, and the input itself discarded once something after it is.

[fn:ref] Often by reference!
[fn:one-sided] In principle we could restrict exchange in only one direction, resulting in a one-way "barrier" to reorders.
**** TODO figures
*** Reductions
We've already seen how the properties of names allow for efficient resource instantiation and combination. Unfortunately, the efficiency ultimately relies on identifying equivalent inputs, which is not always cheap and sometimes impossible. Consider the compute pipeline. A "run of the pipeline" might depend on the entire pipeline package and then project out the executable for each stage. Since you've changed one module in the pipeline, the whole package has changed. If your change only impacts, say, the last stage of the pipeline, the individual stages might be able to recognize that their executables are unchanged. But after the first stage, this recognition wouldn't result in reuse: the first stage may have output cached results, but other stages may not be able to cheaply detect that the output is the same and so would have to rerun. *Reductions*[fn:2-cell] allow us to convey this kind of information by relating one name to another; once we know that "build the project and project out the first executable" reduces to "this particular named executable resource", we can apply our caching logic to the entire composed chain without ever running any particular unchanged stage:

[FIG]

Reductions compose with each other, including across substitutions and projections[fn:2-comp]; they can be thought of as substitutions at the name level. For example, if we have:

[FIG]

Then we get a composite reduction:

[FIG]

Reductions must preserve determinism. Some trivial reductions come automatically: Any depth of nested substitutions reduces to a substitution where everything is simultaneous[fn:lax], contraction followed by weakening on one of the outputs cancels out to a noop, and a sequence of exchanges that leaves you back where you started cancels. Others are domain-specific, letting you express how your names relate to other names.

Reductions can be determined a priori, just based on the name, or can be identified while the name is being run; a compilation name might run the compilation to completion and then reduce itself to a content-addressed name for the resultant file.

Reductions can effectively change the input requirements; we can drop, duplicate, or rearrange wires so long as we respect substructral restrictions[fn:red-substruct]. Reductions can also *downcast* output types into a more specific type, if we know that the output in the specific cases we've isolated will actually be the right kind of resource. Together, these capabilities allow us to flexibly build names that reuse other names for their work and make that reuse visible to the system as a whole. For example, we could build a TTL cache combinator that takes some name and produces a new name that takes all the same inputs plus the current time and cache state, and either reduces to some named resources (weakening the remaining inputs) if we've run this name recently enough or reduces to the underlying name with the remaining inputs if we haven't (and captures the result for next time)[fn:ml]. Or all of our names that deal with files could delegate the actual file storage to some content-based names and downcast the results to an appropriate specific kind of file, allowing us to identify two different names that result in a file with the same contents as being the same.

[fn:2-cell] The 2-cells. Note that each hom-category is thin for our purposes, i.e. the only relevant 2-dimensional data is whether a reduction exists in a given direction or not
[fn:2-comp] (Unbiased) composition of 2-cells, including vertical, horizontal, and tensoring
[fn:lax] Thus our 1-composition is lax, not even weak
[fn:red-substruct] In particular, we can't drop a relevant wire unless we already used the resource before identifying/following the reduction, we can't retain an affine wire unless we haven't used it before identifying/following the reduction, and the evident but verbose rules for ordered wires apply as well.
[fn:ml] Note that this could be arbitrarily complex; we could e.g. have some ML-based "fuzzy matching" on the inputs and an extra model state input, if we have some learned notion of when results are going to be "close enough" based on the input closeness.
*** Namepsaces
Implicit in the whole discussion so far is that we are describing an open system: you can freely add new resource types, new names, new reductions, so long as they meet the requirements. Unfortunately, proving or enforcing those requirements is in general infeasible. As a result, the system as a whole is conceptually partitioned into multiple *namespaces*, each of which has control over the names and reductions within it but cannot influence names in namespaces that don't (transitively) reference it. You may have set up a namespace for managing your local project checkouts (so you can just tell your editor "open this file in that project"), and that namespace impacts the module compilation that uses the files you edited by determining which files are passed on to the compiler, but outside of names that reference local project names the module compilation namespace is isolated from anything the local project namespace does, including any properties it violates.

Namespaces are also the locus of caching, including distributed caching and reductions. Namespaces can keep previous results in a *store* or *forward* results from another namespace (say, on another machine). A namespace can also identify reductions for any of its names.

In order to have caching/reduction for composite names whose substitutions cross namespace boundaries, we need some way to determine which namespace gets to provide the results or identify the substitutions as well as some trusted mechanism for that namespace to do name equality comparisons from different namespaces. For the first issue, we reduce the name to a fully flattened normal form and work backwards from the final outputs, letting the relevant namespace determine if it knows of a reduction or has a cached result for the whole input graph up to that point[fn:anywhere]. For the second issue, we can treat namespaces as a resource type and introduce a *namespace of namespaces*, i.e. a namespace whose names produce namespaces. Then each namespace can have namespaces it trusts to name other namespaces, and confirm with the trusted namespaces that a given name belongs where it claims. This can also be used for overlaying optimization or instrumentation; we might have a namespace of namespaces that says "for any name in the namespaces I expose, I'm first going to check this reduction cache I trust to see if it reduces, and only forward on to the underlying namespace if not", which would among other things allow different users on the same machine to have their own trusted 3rd party caches without requiring mutual trust. This can also be used to bootstrap the system; much like filenames are usually releative to some ambient root or current directory, most names will be relative to some ambient namespace namespace that provides the default set of namespaces for the user or the system.

[fn:anywhere] Technically we could safely allow namespaces to reduce based on what comes /after/ as well. But until a use case arises this allows for a much more straightforward and efficient execution algorithm.
** L Core technical components/architecture implementing the model
+ GC
+ Centralize reductions/per user
+ Lazy/incomplete
+ Value-like resources
+ Pull/push
+ Provisional results
+ Scheduler/resource optimization/global optimization
+ Priority queue caps
+ Well-known projections
+ Naming
  + Hashing vs authoritative name server, what to hash
** M Potential applications (general, not Scarf specific)
*** Prior work
+ CAS (git, IPFS)
+ Nix
  + Enforcement note
  + Reduction for fixed output
+ Unison
+ Nelson
+ Cross comp
+ Modules/functions/computation (Unison)
+ Pipeline
+ Packages
+ Services
  + Restart vs reconfig
+ Compliation
+ Memory map
+ Secrets capabilitial
** S Engineering standards/technical philosophy of the implementation
+ Spec
+ Composable (lib/framework)
  + Mechanism vs policy (semantic)
+ O11Y (dynamic adjustment)
+ Verification
+ Caps (resource limits)
+ Poly/mono repo, schemas
+ flags
+ Protocol enhancement
* Scarf porcelain
** S Why Elvysh is the right basis for Scarf's tooling
+ Provide a package distribution channel that collected usage statistics for maintainers by default (better-informed maintainers -> better software, enables business decisions around OSS)
+ Give OSS authors enough leverage over their own code to meaningfully charge the companies that rely on it
+ Provide a commercial platform for OSS delivery to commercial users, by offering native payments, paid licensing, premium feature delivery, etc
+ Provide a unified package management experience across different systems
+ Align dev tools around maintainers
+ O11Y → metrics
** S Why scarf is good for elvysh
** M Potential functionality and use cases of frontend(s)
+ Command not found/implicit env (w/locking?)
*** Match domain-specific tooling
** M Expected user knowledge/background for various use cases
(incl setup/config)
** M Expected interface with Elvysh core
** M Accompanying infrastructure
* Project plan
** L Roadmap with technical and functional milestones
Nixpkgs compat:
  Add files
    direct add to store
    Builtin drvs
    recursive vs flat
  References
  Run drvs
    Basic execution
    Funky special features
    Serialize drvs
    Intensional?
    Recursive?
    Remote?
    Substitution?
  GC
  nixexpr interface
    Basic eval
    String context
    path
    derivationStrict
    funky builtins?
    Interface to other stores?
  nixenv/profile interface
    GC connected to profile dirs
Haskell
  Individual module
  Whole package
  Deps?
  nix bidi interaction
Interface
  C
  Rust
  Haskell
Documentation
  Reference/protocols
  Tutorials
  Cookbook/how-to
Formal modelling
Portability?
** L Detailed review of each phase
** L Timelines
** S Opportunities for parallelism/team work
** M Proposal for messaging/marketing to existing Nix and developer tool communities
** M Expected limitations of each milestone and the completed initial product
** S Future opportunities
* S Proposed terms of employment
** Governance
Owner's interest, maintainers decisision

7S, 8M, 5L
